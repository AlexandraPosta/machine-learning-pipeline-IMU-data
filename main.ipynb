{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMU Data Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Data processing\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Helper functions\n",
    "from helper.helper_filter import *\n",
    "from helper.helper_preprocess import *\n",
    "from helper.helper_train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data from each target action and store them in a list\n",
    "lqw_raw = load_data(\"./IMU_Data/LGW\")\n",
    "ramp_ascend_raw = load_data(\"./IMU_Data/Ramp_ascend\")\n",
    "ramp_descend_raw = load_data(\"./IMU_Data/Ramp_descend\")\n",
    "sit_to_stand_raw = load_data(\"./IMU_Data/Sit_to_stand\")\n",
    "stand_to_sit_raw = load_data(\"./IMU_Data/Stand_to_sit\")\n",
    "\n",
    "folders = [lqw_raw, ramp_ascend_raw, ramp_descend_raw, sit_to_stand_raw, stand_to_sit_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns that contain sync, annotations and offset timestamps\n",
    "for folder in folders:\n",
    "    for file in folder:\n",
    "        # Drop all columns that contain sync, annotations and offset timestamps\n",
    "        file.data_filtered.drop(columns=[col for col in file.data_filtered.columns if \n",
    "                                any(info in col.lower() for info in [\"sync\", \"offset\", \"annotation\"])], inplace=True)\n",
    "        \n",
    "        # Drop all timestamp columns that are not \"Shank_L_Timestamp\"\n",
    "        for column in file.data_filtered.columns:\n",
    "            if \"timestamp\" in column.lower():\n",
    "                if column.lower() != \"shank_l_timestamp\":\n",
    "                    file.data_filtered.drop(columns=column, inplace=True)\n",
    "        \n",
    "        # Replace column name and place as the first index \n",
    "        file.data_filtered.rename(columns={'Shank_L_Timestamp': 'Timestamp'}, inplace=True)\n",
    "        col = file.data_filtered.pop('Timestamp')\n",
    "        file.data_filtered.insert(0, col.name, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with the k-Nearest Neighbor\n",
    "for folder in folders:\n",
    "    for file in folder:\n",
    "        if file.data_filtered.isnull().sum().sum() > 0:\n",
    "            imputer = KNNImputer(n_neighbors=5)\n",
    "            file.data_filtered = pd.DataFrame(imputer.fit_transform(file.data_filtered), \n",
    "                                              columns = file.data_filtered.columns)\n",
    "            \n",
    "# Check if any NaN values are left\n",
    "for folder in folders:\n",
    "    for file in folder:\n",
    "        if file.data_filtered.isnull().sum().sum() > 0: \n",
    "            print(\"NaN values left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers and smooth curve using a low pass filter\n",
    "for folder in folders:\n",
    "    for file in folder:\n",
    "        # Extract sampling time\n",
    "        ts = file.data_filtered[\"Timestamp\"].diff().median() # Median sampling time\n",
    "\n",
    "        # Remove outliers\n",
    "        for name, data in file.data_filtered.items():\n",
    "            if name != 'Timestamp':\n",
    "                data = low_pass_filter(ts, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the slinding window technique\n",
    "tw = 350        # window size\n",
    "dt = 50         # window step\n",
    "\n",
    "# Apply the moving average filter to the data and get all features\n",
    "for folder in folders:\n",
    "    for file in folder:\n",
    "        # Apply the slinding window to the data\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "            file.data_processed = generate_features(file.data_filtered, tw, dt)\n",
    "\n",
    "        # Drop first row where the gradient is 0\n",
    "        file.data_processed = file.data_processed.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all five actions into one dataframe and set the target labels using one-hot encoding \n",
    "iterator = 1\n",
    "all_df = []\n",
    "\n",
    "for folder in folders:    \n",
    "    # Create single dataframe for action\n",
    "    df = pd.DataFrame()\n",
    "    df = pd.concat([file.data_processed for file in folder[:2]])\n",
    "\n",
    "    # Add target labels\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "        df[\"Action\"] = iterator\n",
    "        iterator = iterator + 1\n",
    "    \n",
    "    # Add dataframe to the list\n",
    "    all_df.append(df)\n",
    "\n",
    "# Combine all dataframes into one\n",
    "df = pd.concat(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"combined_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = df.iloc[:, :-1]     # Input features\n",
    "y = df.iloc[:, -1:]     # Target labels\n",
    "\n",
    "# Split data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=109) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN\n",
    "ann = ANN(X_train, y_train, X_test, y_test)\n",
    "ann.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm = SVM(X_train, y_train, X_test, y_test)\n",
    "svm.run_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 most relevant features ANN\n",
    "ann_features, ann_scores = ann.get_most_relevant_features(X_train.columns,\n",
    "                                                          X_train.values,\n",
    "                                                          y_train.values)\n",
    "\n",
    "# Plot\n",
    "plt.barh(ann_features, ann_scores)\n",
    "plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 most relevant features SVM\n",
    "svm_features, svm_scores = svm.get_most_relevant_features(X_train.columns,\n",
    "                                                          X_train.values,\n",
    "                                                          y_train.values)\n",
    "\n",
    "# Plot \n",
    "plt.barh(svm_features, svm_scores)\n",
    "plt.xlabel(\"Permutation Importance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
