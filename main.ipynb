{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMU Data Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Data processing\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Helper functions\n",
    "from helper.helper_filter import *\n",
    "from helper.helper_preprocess import *\n",
    "from helper.helper_train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the raw data from each target action and store them in a list\n",
    "lqw_raw = load_data(\"./IMU_Data/LGW\")\n",
    "ramp_ascend_raw = load_data(\"./IMU_Data/Ramp_ascend\")\n",
    "ramp_descend_raw = load_data(\"./IMU_Data/Ramp_descend\")\n",
    "sit_to_stand_raw = load_data(\"./IMU_Data/Sit_to_stand\")\n",
    "stand_to_sit_raw = load_data(\"./IMU_Data/Stand_to_sit\")\n",
    "\n",
    "folders = [lqw_raw, ramp_ascend_raw, ramp_descend_raw, sit_to_stand_raw, stand_to_sit_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all columns that contain sync, annotations and offset timestamps\n",
    "for folder in folders:\n",
    "    for file in folder:\n",
    "        # Drop all columns that contain sync, annotations and offset timestamps\n",
    "        file.data_filtered.drop(columns=[col for col in file.data_filtered.columns if \n",
    "                                any(info in col.lower() for info in [\"sync\", \"offset\", \"annotation\"])], inplace=True)\n",
    "        \n",
    "        # Drop all timestamp columns that are not \"Shank_L_Timestamp\"\n",
    "        for column in file.data_filtered.columns:\n",
    "            if \"timestamp\" in column.lower():\n",
    "                if column.lower() != \"shank_l_timestamp\":\n",
    "                    file.data_filtered.drop(columns=column, inplace=True)\n",
    "        \n",
    "        # Replace column name and place as the first index \n",
    "        file.data_filtered.rename(columns={'Shank_L_Timestamp': 'Timestamp'}, inplace=True)\n",
    "        col = file.data_filtered.pop('Timestamp')\n",
    "        file.data_filtered.insert(0, col.name, col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN values with the k-Nearest Neighbor\n",
    "for folder in folders:\n",
    "    for file in folder:\n",
    "        if file.data_filtered.isnull().sum().sum() > 0:\n",
    "            imputer = KNNImputer(n_neighbors=5)\n",
    "            file.data_filtered = pd.DataFrame(imputer.fit_transform(file.data_filtered), \n",
    "                                              columns = file.data_filtered.columns)\n",
    "            \n",
    "# Check if any NaN values are left\n",
    "for folder in folders:\n",
    "    for file in folder:\n",
    "        if file.data_filtered.isnull().sum().sum() > 0: \n",
    "            print(\"NaN values left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove outliers and smooth curve using a low pass filter\n",
    "for folder in folders:\n",
    "    for file in folder:\n",
    "        # Extract sampling time\n",
    "        ts = file.data_filtered[\"Timestamp\"].diff().median() # Median sampling time\n",
    "\n",
    "        # Remove outliers\n",
    "        for name, data in file.data_filtered.items():\n",
    "            if name != 'Timestamp':\n",
    "                data = low_pass_filter(ts, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the slinding window technique\n",
    "tw = 350        # window size\n",
    "dt = 50         # window step\n",
    "\n",
    "# Apply the moving average filter to the data and get all features\n",
    "for folder in folders:\n",
    "    for file in folder:\n",
    "        # Apply the slinding window to the data\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "            file.data_processed = generate_features(file.data_filtered, tw, dt)\n",
    "\n",
    "        # Drop first row where the gradient is 0\n",
    "        file.data_processed = file.data_processed.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all five actions into one dataframe and set the target labels using one-hot encoding \n",
    "iterator = 1\n",
    "all_df = []\n",
    "\n",
    "for folder in folders:    \n",
    "    # Create single dataframe for action\n",
    "    df = pd.DataFrame()\n",
    "    df = pd.concat([file.data_processed for file in folder[:2]])\n",
    "\n",
    "    # Add target labels\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=pd.errors.PerformanceWarning)\n",
    "        df[\"Action\"] = iterator\n",
    "        iterator = iterator + 1\n",
    "    \n",
    "    # Add dataframe to the list\n",
    "    all_df.append(df)\n",
    "\n",
    "# Combine all dataframes into one\n",
    "df = pd.concat(all_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"combined_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = df.iloc[:, :-1]     # Input features\n",
    "y = df.iloc[:, -1:]     # Target labels\n",
    "\n",
    "# Split data into training and testing set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=109) # 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANN\n",
    "ann = ANN(X_train, y_train, X_test, y_test)\n",
    "ann.run_pipeline()\n",
    "ann.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "svm = SVM(X_train, y_train, X_test, y_test)\n",
    "svm.run_pipeline()\n",
    "svm.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import (Conv1D, Dense, Flatten)\n",
    "\n",
    "def create_model(X_tra, y_tra, X_tes, y_tes):\n",
    "    n_timesteps, n_features, n_outputs = 1003, 441, 6\n",
    "\n",
    "    # Init model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add layers\n",
    "    model.add(Conv1D(filters=32, kernel_size=2, activation='relu', input_shape=(n_features, 1), padding='same'))\n",
    "    model.add(Flatten())   # 1D array\n",
    "    model.add(Dense(n_outputs, activation='softmax'))    \n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_tra, y_tra, epochs=10, batch_size=32, verbose=2, validation_data=(X_tes, y_tes))\n",
    "    return model, history\n",
    "\n",
    "model, hist = create_model(X_train.to_numpy().reshape(-1, 441, 1), y_train.to_numpy(), X_test.to_numpy().reshape(-1, 441, 1), y_test.to_numpy())\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 most relevant features ANN\n",
    "ann_features, ann_scores = ann.get_most_relevant_features(X_train.columns,\n",
    "                                                          X_train.values,\n",
    "                                                          y_train.values)\n",
    "\n",
    "# Plot\n",
    "plt.barh(ann_features, ann_scores)\n",
    "plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15 most relevant features SVM\n",
    "svm_features, svm_scores = svm.get_most_relevant_features(X_train.columns,\n",
    "                                                          X_train.values,\n",
    "                                                          y_train.values)\n",
    "\n",
    "# Plot \n",
    "plt.barh(svm_features, svm_scores)\n",
    "plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate data based on the segment\n",
    "sensors = [\"Foot_L\", \"Foot_R\", \"Thigh_L\", \"Thigh_R\", \"Shank_L\", \"Shank_L\", \"Pelvis\"]\n",
    "sensors_data = {}\n",
    "\n",
    "# Get all columns that contain the sensor name\n",
    "for sensor in sensors:\n",
    "    columns = [col for col in df.columns if (sensor in col or \"Action\" in col)]\n",
    "    sensors_data[sensor] = df[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the highest accuracy \n",
    "models = []\n",
    "\n",
    "# Get the accuracy of the ANN on each separate data\n",
    "for key, data in sensors_data.items():\n",
    "    # Split data\n",
    "    X = data.iloc[:, :-1]     # Input features\n",
    "    y = data.iloc[:, -1:]     # Target labels\n",
    "\n",
    "    # Split data into training (70%) and testing set (30%)\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "\n",
    "    # Create network and get accuracy\n",
    "    sensor_ann = ANN(X_tr, X_te, y_tr, y_te)\n",
    "    sensor_ann.run_pipeline()\n",
    "\n",
    "    # Add model to the list\n",
    "    models.append(ModelToCompare(key , sensor_ann.get_accuracy(), sensor_ann, X_tr, X_te, y_tr, y_te))\n",
    "\n",
    "# Check highest accuacy\n",
    "max_acc_model = max(models, key=lambda item: item.accuracy) \n",
    "max_acc_model.model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only the most relevant feature, calculate the clasification error using ANN\n",
    "print(f\"Classification error of the ANN using {max_acc_model.sensor}: {max_acc_model.model.get_classification_error()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using only the most relevant feature, calculate the clasification error using SVM\n",
    "sensor = sensors_data[max_acc_model.sensor]\n",
    "\n",
    "# Split data\n",
    "X = sensor.iloc[:, :-1]     # Input features\n",
    "y = sensor.iloc[:, -1:]     # Target labels\n",
    "\n",
    "# Split data into training (70%) and testing set (30%)\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.7, random_state=42)\n",
    "\n",
    "max_svm_model = SVM(X_tr, X_te, y_tr, y_te)\n",
    "max_svm_model.run_pipeline()\n",
    "print(f\"Accuracy of the SVM using {max_acc_model.sensor}: {max_svm_model.get_accuracy()}\")\n",
    "print(f\"Classification error of the SVM using {max_acc_model.sensor}: {max_svm_model.get_classification_error()}\")\n",
    "max_svm_model.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
